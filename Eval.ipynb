{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn --no-build-isolation\n",
        "!pip install flash-linear-attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmoz-lGWt2EA",
        "outputId": "d9cbdffd-040d-4660-fd6b-be69d72765be"
      },
      "id": "cmoz-lGWt2EA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.8.3.tar.gz (8.4 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/8.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/8.4 MB\u001b[0m \u001b[31m173.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m189.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.3)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl size=256040057 sha256=f25da18657a87fc83dc1bfb8b7751b82246e9db355510226b674fd437c34b5fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/59/46/f282c12c73dd4bb3c2e3fe199f1a0d0f8cec06df0cccfeee27\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.8.3\n",
            "Collecting flash-linear-attention\n",
            "  Downloading flash_linear_attention-0.4.1-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting fla-core==0.4.1 (from flash-linear-attention)\n",
            "  Downloading fla_core-0.4.1-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from flash-linear-attention) (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from fla-core==0.4.1->flash-linear-attention) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from fla-core==0.4.1->flash-linear-attention) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers->flash-linear-attention) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers->flash-linear-attention) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers->flash-linear-attention) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers->flash-linear-attention) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->flash-linear-attention) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->flash-linear-attention) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->flash-linear-attention) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->flash-linear-attention) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->flash-linear-attention) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->flash-linear-attention) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->flash-linear-attention) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->flash-linear-attention) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->flash-linear-attention) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->flash-linear-attention) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->flash-linear-attention) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->flash-linear-attention) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->flash-linear-attention) (2025.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->fla-core==0.4.1->flash-linear-attention) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->fla-core==0.4.1->flash-linear-attention) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->fla-core==0.4.1->flash-linear-attention) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->fla-core==0.4.1->flash-linear-attention) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->fla-core==0.4.1->flash-linear-attention) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->fla-core==0.4.1->flash-linear-attention) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->fla-core==0.4.1->flash-linear-attention) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->fla-core==0.4.1->flash-linear-attention) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->fla-core==0.4.1->flash-linear-attention) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->fla-core==0.4.1->flash-linear-attention) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->fla-core==0.4.1->flash-linear-attention) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->fla-core==0.4.1->flash-linear-attention) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->fla-core==0.4.1->flash-linear-attention) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->fla-core==0.4.1->flash-linear-attention) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->fla-core==0.4.1->flash-linear-attention) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->fla-core==0.4.1->flash-linear-attention) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->fla-core==0.4.1->flash-linear-attention) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->fla-core==0.4.1->flash-linear-attention) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->fla-core==0.4.1->flash-linear-attention) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->fla-core==0.4.1->flash-linear-attention) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->fla-core==0.4.1->flash-linear-attention) (3.0.3)\n",
            "Downloading flash_linear_attention-0.4.1-py3-none-any.whl (287 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.4/287.4 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fla_core-0.4.1-py3-none-any.whl (437 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.3/437.3 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fla-core, flash-linear-attention\n",
            "Successfully installed fla-core-0.4.1 flash-linear-attention-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from datasets import load_dataset\n",
        "\n",
        "from transformers import AutoModelForCausalLM, PreTrainedTokenizerFast\n",
        "\n",
        "import fla\n",
        "from fla.models import mesa_net  # <-- Add this line\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_ID = \"ChavyvAkvar/mesanet-baseline-1-epoch\"\n",
        "DATASET_ID = \"kreasof-ai/ECA-Zero\"\n",
        "BATCH_SIZE = 128\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# From the dataset generation script\n",
        "WOLFRAM_CLASSES_MAP = {\n",
        "    1: [0, 8, 32, 40, 128, 136, 160, 168],\n",
        "    2: [1, 19, 23, 29, 37, 50, 108, 178],\n",
        "    3: [30, 45, 60, 90, 105, 126, 150],\n",
        "    4: [54, 106, 110, 124, 137, 147, 193]\n",
        "}\n",
        "\n",
        "# Invert for fast lookup: Rule -> Class\n",
        "RULE_TO_CLASS = {}\n",
        "for cls, rules in WOLFRAM_CLASSES_MAP.items():\n",
        "    for r in rules:\n",
        "        RULE_TO_CLASS[r] = cls\n",
        "\n",
        "class ECAVerifier:\n",
        "    def __init__(self):\n",
        "        self.re_rule = re.compile(r\"Rule: (\\d+)\")\n",
        "        self.re_start = re.compile(r\"Start: ([01]+)\")\n",
        "        self.re_end = re.compile(r\"End: ([01]+)\")\n",
        "        self.re_steps = re.compile(r\"Steps: (\\d+)\")\n",
        "        self.re_hint_class = re.compile(r\"Hint: Class (\\d)\")\n",
        "        self.re_tt = re.compile(r\"([01]{3})->([01])\")\n",
        "\n",
        "    def get_wolfram_class(self, prompt):\n",
        "        # 1. Check for explicit Hint (Induction tasks)\n",
        "        m = self.re_hint_class.search(prompt)\n",
        "        if m:\n",
        "            return int(m.group(1))\n",
        "\n",
        "        # 2. Check for Rule ID (Deduction/Abduction) and look up\n",
        "        m = self.re_rule.search(prompt)\n",
        "        if m:\n",
        "            rule = int(m.group(1))\n",
        "            return RULE_TO_CLASS.get(rule, 0) # 0 = Unknown/Other\n",
        "\n",
        "        return 0\n",
        "\n",
        "    def get_next_state(self, state, rule):\n",
        "        next_state = []\n",
        "        L = len(state)\n",
        "        for i in range(L):\n",
        "            l, c, r = state[(i - 1) % L], state[i], state[(i + 1) % L]\n",
        "            pattern = (l << 2) | (c << 1) | r\n",
        "            bit = 1 if (rule & (1 << pattern)) else 0\n",
        "            next_state.append(bit)\n",
        "        return next_state\n",
        "\n",
        "    def simulate(self, start_state, rule, steps):\n",
        "        current = list(start_state)\n",
        "        for _ in range(steps):\n",
        "            current = self.get_next_state(current, rule)\n",
        "        return current\n",
        "\n",
        "    def parse_rule_string(self, text):\n",
        "        matches = self.re_tt.findall(text)\n",
        "        if not matches: return None\n",
        "        rule = 0\n",
        "        for pat, res in matches:\n",
        "            if res == '1': rule |= (1 << int(pat, 2))\n",
        "        return rule\n",
        "\n",
        "    def verify(self, task_type, prompt, model_output_str):\n",
        "        try:\n",
        "            steps = int(self.re_steps.search(prompt).group(1))\n",
        "            start_match = self.re_start.search(prompt)\n",
        "            start_state = [int(x) for x in start_match.group(1)] if start_match else None\n",
        "            end_match = self.re_end.search(prompt)\n",
        "            end_state = [int(x) for x in end_match.group(1)] if end_match else None\n",
        "            rule_match = self.re_rule.search(prompt)\n",
        "            rule = int(rule_match.group(1)) if rule_match else None\n",
        "        except AttributeError:\n",
        "            return False\n",
        "\n",
        "        answer = model_output_str.strip()\n",
        "        try:\n",
        "            if task_type == 'deduction':\n",
        "                pred_state = [int(x) for x in answer if x in '01']\n",
        "                if not pred_state: return False\n",
        "                expected = self.simulate(start_state, rule, steps)\n",
        "                return pred_state == expected\n",
        "\n",
        "            elif task_type == 'induction':\n",
        "                pred_rule = self.parse_rule_string(answer)\n",
        "                if pred_rule is None: return False\n",
        "                sim_end = self.simulate(start_state, pred_rule, steps)\n",
        "                return sim_end == end_state\n",
        "\n",
        "            elif task_type == 'abduction':\n",
        "                pred_start = [int(x) for x in answer if x in '01']\n",
        "                if not pred_start or len(pred_start) != len(end_state): return False\n",
        "                sim_end = self.simulate(pred_start, rule, steps)\n",
        "                return sim_end == end_state\n",
        "        except Exception:\n",
        "            return False\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    print(f\"Loading tokenizer from {MODEL_ID}...\")\n",
        "    try:\n",
        "        tokenizer = PreTrainedTokenizerFast.from_pretrained(MODEL_ID)\n",
        "    except:\n",
        "        from transformers import AutoTokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(f\"Loading model from {MODEL_ID}...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=DEVICE,\n",
        "    )\n",
        "    print(\"Compiling the model\")\n",
        "    model = torch.compile(model)\n",
        "    model.eval()\n",
        "\n",
        "    print(\"Loading Test Set...\")\n",
        "    dataset = load_dataset(DATASET_ID, split=\"test\")\n",
        "    verifier = ECAVerifier()\n",
        "\n",
        "    # Storage: results[task][class_id] = [True, False, ...]\n",
        "    results = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "    print(\"Starting Stratified Evaluation...\")\n",
        "\n",
        "    for i in tqdm(range(0, len(dataset), BATCH_SIZE)):\n",
        "        batch = dataset[i : i + BATCH_SIZE]\n",
        "        tasks = batch['task']\n",
        "        inputs = batch['input']\n",
        "\n",
        "        prompts = [f\"{tokenizer.bos_token}{inp}\\n<think>\\n\" for inp in inputs]\n",
        "\n",
        "        # FIX: Added return_token_type_ids=False\n",
        "        encodings = tokenizer(\n",
        "            prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=2048,\n",
        "            return_token_type_ids=False,\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                input_ids=encodings['input_ids'],\n",
        "                max_new_tokens=2048,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        decoded_outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)\n",
        "\n",
        "        for j, raw_output in enumerate(decoded_outputs):\n",
        "            if \"</think>\" in raw_output:\n",
        "                final_answer = raw_output.split(\"</think>\")[-1].replace(tokenizer.eos_token, \"\").strip()\n",
        "            else:\n",
        "                final_answer = \"\"\n",
        "\n",
        "            # Determine Class\n",
        "            w_class = verifier.get_wolfram_class(inputs[j])\n",
        "\n",
        "            # Verify\n",
        "            is_correct = verifier.verify(tasks[j], inputs[j], final_answer)\n",
        "\n",
        "            # Store\n",
        "            results[tasks[j]][w_class].append(is_correct)\n",
        "            results[tasks[j]][\"ALL\"].append(is_correct)\n",
        "\n",
        "    # --- Print Report ---\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STRATIFIED RESULTS (Accuracy by Wolfram Class)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Define column headers\n",
        "    print(f\"{'Task':<12} | {'Class 1':<10} | {'Class 2':<10} | {'Class 3':<10} | {'Class 4':<10} | {'OVERALL':<10}\")\n",
        "    print(\"-\" * 75)\n",
        "\n",
        "    for task in [\"deduction\", \"induction\", \"abduction\"]:\n",
        "        row_str = f\"{task.capitalize():<12} | \"\n",
        "\n",
        "        for c in [1, 2, 3, 4]:\n",
        "            outcomes = results[task][c]\n",
        "            if outcomes:\n",
        "                acc = sum(outcomes) / len(outcomes)\n",
        "                row_str += f\"{acc:.1%} ({len(outcomes):<3}) | \" # concise\n",
        "            else:\n",
        "                row_str += \"N/A        | \"\n",
        "\n",
        "        # Overall\n",
        "        all_outcomes = results[task][\"ALL\"]\n",
        "        if all_outcomes:\n",
        "            total_acc = sum(all_outcomes) / len(all_outcomes)\n",
        "            row_str += f\"{total_acc:.1%} ({len(all_outcomes)})\"\n",
        "\n",
        "        print(row_str)\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"Class Legend:\")\n",
        "    print(\"1: Uniform (Trivial) | 2: Periodic (Easy) | 3: Chaotic (Hard) | 4: Complex (Hardest)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "bOU8pai7TSmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35a9cc34-6afa-43d3-9f5b-af1a2e75f56e"
      },
      "id": "bOU8pai7TSmL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer from ChavyvAkvar/mesanet-baseline-1-epoch...\n",
            "Loading model from ChavyvAkvar/mesanet-baseline-1-epoch...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling the model\n",
            "Loading Test Set...\n",
            "Starting Stratified Evaluation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/27 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "This is a friendly reminder - the current text generation call has exceeded the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
            "  4%|▎         | 1/27 [05:17<2:17:34, 317.48s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "  7%|▋         | 2/27 [10:22<2:09:07, 309.88s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 11%|█         | 3/27 [15:26<2:02:56, 307.37s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 15%|█▍        | 4/27 [20:31<1:57:25, 306.32s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 19%|█▊        | 5/27 [25:36<1:52:09, 305.87s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 22%|██▏       | 6/27 [30:41<1:46:55, 305.52s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 26%|██▌       | 7/27 [35:44<1:41:38, 304.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 30%|██▉       | 8/27 [40:49<1:36:33, 304.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 33%|███▎      | 9/27 [46:05<1:32:31, 308.39s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 37%|███▋      | 10/27 [51:12<1:27:11, 307.75s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 41%|████      | 11/27 [56:18<1:21:55, 307.24s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 44%|████▍     | 12/27 [1:01:23<1:16:38, 306.54s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 48%|████▊     | 13/27 [1:06:28<1:11:28, 306.34s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 52%|█████▏    | 14/27 [1:11:34<1:06:17, 305.99s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 56%|█████▌    | 15/27 [1:16:40<1:01:14, 306.19s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 59%|█████▉    | 16/27 [1:21:46<56:07, 306.17s/it]  A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 63%|██████▎   | 17/27 [1:26:52<50:59, 305.95s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 67%|██████▋   | 18/27 [1:31:56<45:47, 305.32s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 70%|███████   | 19/27 [1:37:02<40:44, 305.50s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 74%|███████▍  | 20/27 [1:42:07<35:39, 305.60s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 78%|███████▊  | 21/27 [1:47:14<30:35, 305.94s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 81%|████████▏ | 22/27 [1:52:20<25:29, 305.80s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 85%|████████▌ | 23/27 [1:57:26<20:23, 305.91s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 89%|████████▉ | 24/27 [2:02:31<15:16, 305.55s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 93%|█████████▎| 25/27 [2:07:37<10:11, 305.95s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 96%|█████████▋| 26/27 [2:12:42<05:05, 305.69s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "100%|██████████| 27/27 [2:16:05<00:00, 302.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STRATIFIED RESULTS (Accuracy by Wolfram Class)\n",
            "============================================================\n",
            "Task         | Class 1    | Class 2    | Class 3    | Class 4    | OVERALL   \n",
            "---------------------------------------------------------------------------\n",
            "Deduction    | 14.2% (113) | 11.5% (226) | 11.9% (412) | 12.4% (410) | 12.2% (1161)\n",
            "Induction    | 0.0% (113) | 1.8% (227) | 1.0% (414) | 0.2% (411) | 0.8% (1165)\n",
            "Abduction    | 0.0% (47 ) | 0.0% (185) | 0.0% (388) | 0.0% (387) | 0.0% (1007)\n",
            "============================================================\n",
            "Class Legend:\n",
            "1: Uniform (Trivial) | 2: Periodic (Easy) | 3: Chaotic (Hard) | 4: Complex (Hardest)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "ECA Zero Eval",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}